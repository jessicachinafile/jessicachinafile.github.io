from bs4 import BeautifulSoup
import requests
from pprint import pprint
import re



# a function to remove all the extra crap regex adds to lines -- here's where I found how to do that: https://www.tutorialspoint.com/How-to-remove-specific-characters-from-a-string-in-Python
def regexcleaner(rawstring) :
    return str((re.sub("\'|\]|\\[", "", rawstring)))

city_url = 'https://web.archive.org/web/20171004155730/http://ldzl.people.com.cn/dfzlk/front/chengqu1029.htm'

city_page = requests.get(city_url)
city_soup = BeautifulSoup(city_page.text, 'lxml')

###################################################################################################
#####THIS HARVESTS THE CITY-LEVEL INDIVIDUALS' INFORMATION AT THE TOP OF THE PAGE ######
###################################################################################################


# This pulls the person's name and URL. Here's where I found the code to find <a> as nested under specific parent tags: https://stackoverflow.com/questions/1058599/how-to-get-a-nested-element-in-beautiful-soup
citypage_name_url = [dd.find('a') for dd in city_soup.findAll('dd')]

# Generates the pool by getting the number of entries in the list created by citypage_name_url
citypage_poolpeer = len(citypage_name_url)

# The base rank number -- it will iterate through for each entry in the list and add one, thereby ranking everyone in order
citypage_rankpeer = 1

# Creates empty dictionary we can fill later
citypage_officials_dict = {}

# parses each item in the list generated by citypage_name_url
for item in citypage_name_url :

    # makes each entry in the above list into a string so we can manipulate it
    item_string = str(item)

    # uses regex to just pull the information we're looking for -- must also be converted to a string after regex does its thing
    citypage_urlonly = str(re.findall(r"\"(.*?)\"", item_string))
    # cleans up the regex result with the function defined at top
    citypage_urlonly_cleaned = regexcleaner(citypage_urlonly)

    #makes url complete by adding prefix (internet archive will change date in url to load to last time page was scraped, so we chose a date at end of 2018 to ensure it was post-scraping)
    city_official_full_url = 'https://web.archive.org/web/20181226105432/http://ldzl.people.com.cn:80/dfzlk/front/' + citypage_urlonly_cleaned

    citypage_nameonly = str(re.findall(r"\>(.*?)\</", item_string))
    citypage_nameonly_cleaned = regexcleaner(citypage_nameonly)

    # since we'll have to nest dictionaries, this creates a temporary (nested) dictionary for each iteration of the loop
    holder_dict = {}
    holder_dict["url"] = city_official_full_url
    holder_dict["pool"] = citypage_poolpeer
    holder_dict["rank"] = citypage_rankpeer

    # this creates the big dictionary, that matches each person's name with their temporary dictionary information
    citypage_officials_dict[citypage_nameonly_cleaned] = holder_dict

    # this adds 1 to the rank for the next iteration of the loop
    citypage_rankpeer +=1

pprint(citypage_officials_dict)
print("")


###################################################################################################
#####THIS HARVESTS THE COUNTY-LEVEL INDIVIDUALS' INFORMATION AT THE BOTTOM OF THE PAGE ######
###################################################################################################


# Creates empty dictionary we can fill later
county_dict = {}

# grabs all the information we're interested in in one big chunk
citypage_county_listinfo = city_soup.find_all('ol', {'class' : 'fl'})
#print(str(citypage_county_listinfo) + "\n")

# unfortunately it has only 2 items in it, the first one of which is useless for us (so we ignore it by telling the program to only look at item 1, which is actually item 2)
for item in citypage_county_listinfo[1] :

    # the "items" the previous line of code produces includes a bunch of null/empty items. We ignore those by just telling python to look at items that have "<li>" in them somewhere
    if "<li>" in str(item) :

        # makes sure the item is a string so it can be searched by regex
        item_string = str(item)

        # finds all county names and cleans up the regex result
        county_name_raw = str(re.findall(r"(?<=信息\">).*?(?=<)", item_string))
        county_name = regexcleaner(county_name_raw)

        # finds all info for first person listed per county. This is a necessary first step in harvesting person1's information, as the way the html is written it would be impossible to pull person1 and person2's names separately just by using regex, so we have to break it up into person1 and person2 items, then parse those separately
        county_person1_allinfo = str(re.findall(r"(?<=<em>).*?(?=</em>)", item_string))
        # uses regex to search the variable we just created, then uses function defined at top to clean up the regex result
        county_person1_name_raw = str(re.findall(r"(?<=htm\">).*?(?=</a>)", county_person1_allinfo))
        county_person1_name = regexcleaner(county_person1_name_raw)
        county_person1_url_raw = str(re.findall(r"(?<=href=\").*?(?=\">)", county_person1_allinfo))
        county_person1_url_cleaned = regexcleaner(county_person1_url_raw)
        #makes url complete by adding prefix (internet archive will change date in url to load to last time page was scraped, so we chose a date at end of 2018 to ensure it was post-scraping)
        county_person1_url = 'https://web.archive.org/web/20181226105432/http://ldzl.people.com.cn:80/dfzlk/front/' + county_person1_url_cleaned

        # similar to the variable above, just finds all the info for the second person listed per county
        county_person2_allinfo = str(re.findall(r"(?<=<i>).*?(?=</i>)", item_string))
        county_person2_name_raw = str(re.findall(r"(?<=htm\">).*?(?=</a>)", county_person2_allinfo))
        county_person2_name = regexcleaner(county_person2_name_raw)
        county_person2_url_raw = str(re.findall(r"(?<=href=\").*?(?=\">)", county_person2_allinfo))
        county_person2_url_cleaned = regexcleaner(county_person2_url_raw)
        county_person2_url = 'https://web.archive.org/web/20181226105432/http://ldzl.people.com.cn:80/dfzlk/front/' + county_person2_url_cleaned

        # since we'll have to nest dictionaries, this creates a temporary (nested) dictionary for each iteration of the loop
        holder_dict = {}
        holder_dict["person1"] = county_person1_name
        holder_dict["url1"] = county_person1_url

        # At the county level, there's only ever 2 people listed. so the pool will always be 2, and we can just pre-define that variable rather than having python do any fancy math
        holder_dict["pool1"] = 2

        # because of the way we had to separately pull and name each person's information, we know that the first person will always be ranked 1 and the second person ranked 2. So again, we don't need to do any math here, we can just pre-define these variables
        holder_dict["rank1"] = 1
        holder_dict["person2"] = county_person2_name
        holder_dict["url2"] = county_person2_url
        holder_dict["pool2"] = 2
        holder_dict["rank2"] = 2

        # this creates the big dictionary, that matches each person's name with their temporary dictionary information
        county_dict[county_name] = holder_dict

    # and this is just a thing to balance out the "if" statement earlier in the loop, which is apparently something you're supposed to do or whatever.
    else :
        pass

pprint(county_dict)
